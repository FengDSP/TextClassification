# python3 train.py 
I0124 04:10:33.835203 140626405369664 train.py:615] ----------- Load Data from Cache -----------
I0124 04:10:33.925284 140626405369664 train.py:636] ----------- CPU training -----------
I0124 04:10:33.953931 140626405369664 train.py:639] Total number of parameters for model mlp_e32_512: 1679285
I0124 04:10:34.353103 140626405369664 train.py:478] Epoch 0 step 0.0 loss: 6.646096229553223
I0124 04:10:35.612208 140626405369664 train.py:478] Epoch 0 step 100.0 loss: 5.244625568389893
I0124 04:10:36.872881 140626405369664 train.py:478] Epoch 0 step 200.0 loss: 4.607333183288574
I0124 04:10:38.113877 140626405369664 train.py:478] Epoch 0 step 300.0 loss: 3.929028034210205
I0124 04:10:39.351315 140626405369664 train.py:478] Epoch 0 step 400.0 loss: 3.5070199966430664
I0124 04:10:39.602431 140626405369664 train.py:482] Epoch 0 loss: 3.44268798828125  time: 5.37s
I0124 04:10:39.602676 140626405369664 train.py:648] Evaluating on train set:
I0124 04:10:40.152664 140626405369664 train.py:493] Accuracy: 35.94%
I0124 04:10:40.156679 140626405369664 train.py:651] Evaluating on test set:
I0124 04:10:40.295146 140626405369664 train.py:493] Accuracy: 31.87%
I0124 04:10:40.296451 140626405369664 train.py:655] ----------- CPU training With Customized Linear -----------
I0124 04:10:40.307801 140626405369664 train.py:658] Total number of parameters for model mlp_e32_512: 1679285
I0124 04:10:40.321665 140626405369664 train.py:478] Epoch 0 step 0.0 loss: 6.970399379730225
I0124 04:10:41.492114 140626405369664 train.py:478] Epoch 0 step 100.0 loss: 5.412874221801758
I0124 04:10:42.697826 140626405369664 train.py:478] Epoch 0 step 200.0 loss: 5.009960651397705
I0124 04:10:43.877387 140626405369664 train.py:478] Epoch 0 step 300.0 loss: 4.235611438751221
I0124 04:10:45.049557 140626405369664 train.py:478] Epoch 0 step 400.0 loss: 3.857966423034668
I0124 04:10:45.291930 140626405369664 train.py:482] Epoch 0 loss: 3.797330856323242  time: 4.98s
I0124 04:10:45.292116 140626405369664 train.py:669] ----------- Single GPU training -----------
I0124 04:10:46.169171 140626405369664 train.py:478] Epoch 0 step 0.0 loss: 6.607800483703613
I0124 04:10:46.332968 140626405369664 train.py:478] Epoch 0 step 100.0 loss: 5.218483924865723
I0124 04:10:46.484075 140626405369664 train.py:478] Epoch 0 step 200.0 loss: 4.565174579620361
I0124 04:10:46.630610 140626405369664 train.py:478] Epoch 0 step 300.0 loss: 3.8513362407684326
I0124 04:10:46.777895 140626405369664 train.py:478] Epoch 0 step 400.0 loss: 3.504352331161499
I0124 04:10:46.808339 140626405369664 train.py:482] Epoch 0 loss: 3.462463617324829  time: 1.25s
I0124 04:10:46.808431 140626405369664 train.py:684] ----------- Data Parallel ------------
I0124 04:10:46.808476 140626405369664 train.py:685] GPU count = 8
I0124 04:10:57.985879 140626405369664 train.py:478] Epoch 0 step 0.0 loss: 6.6470232009887695
I0124 04:10:59.465127 140626405369664 train.py:478] Epoch 0 step 100.0 loss: 5.269992351531982
I0124 04:11:00.897559 140626405369664 train.py:478] Epoch 0 step 200.0 loss: 4.6957244873046875
I0124 04:11:02.331494 140626405369664 train.py:478] Epoch 0 step 300.0 loss: 3.9277520179748535
I0124 04:11:03.744328 140626405369664 train.py:478] Epoch 0 step 400.0 loss: 3.5586912631988525
I0124 04:11:04.029070 140626405369664 train.py:482] Epoch 0 loss: 3.517662286758423  time: 17.17s
I0124 04:11:04.029273 140626405369664 train.py:700] ----------- Distributed Data Parallel ------------
P0] Starting ddp_main_fn rank 0 of world_size 4 ...
P2] Starting ddp_main_fn rank 2 of world_size 4 ...
P1] Starting ddp_main_fn rank 1 of world_size 4 ...
P3] Starting ddp_main_fn rank 3 of world_size 4 ...
P1] Epoch 0 step 0.0 loss: 6.603433609008789
P2] Epoch 0 step 0.0 loss: 6.591819763183594
P3] Epoch 0 step 0.0 loss: 6.5975518226623535
P0] Epoch 0 step 0.0 loss: 6.582042217254639
P0] Epoch 0 step 100.0 loss: 5.4312543869018555
P2] Epoch 0 step 100.0 loss: 5.68925142288208
P3] Epoch 0 step 100.0 loss: 5.798285961151123
P1] Epoch 0 step 100.0 loss: 5.690895080566406
P0] Epoch 0 step 200.0 loss: 5.563724994659424
P2] Epoch 0 step 200.0 loss: 5.553706645965576
P3] Epoch 0 step 200.0 loss: 5.789246082305908
P1] Epoch 0 step 200.0 loss: 5.544646739959717
P0] Epoch 0 step 300.0 loss: 5.380524635314941
P2] Epoch 0 step 300.0 loss: 5.426815509796143
P3] Epoch 0 step 300.0 loss: 5.133800983428955
P1] Epoch 0 step 300.0 loss: 5.298405647277832
P0] Epoch 0 step 400.0 loss: 5.041287422180176
P1] Epoch 0 step 400.0 loss: 5.340535640716553
P2] Epoch 0 step 400.0 loss: 5.3877854347229
P3] Epoch 0 step 400.0 loss: 5.141211032867432
P0] Epoch 0 step 500.0 loss: 5.1858601570129395
P1] Epoch 0 step 500.0 loss: 4.877429962158203
P2] Epoch 0 step 500.0 loss: 4.820313453674316
P3] Epoch 0 step 500.0 loss: 4.8636322021484375
P1] Epoch 0 step 600.0 loss: 4.97909688949585
P2] Epoch 0 step 600.0 loss: 4.762006759643555
P0] Epoch 0 step 600.0 loss: 4.789338111877441
P3] Epoch 0 step 600.0 loss: 4.856595993041992
P0] Epoch 0 step 700.0 loss: 4.72007417678833
P1] Epoch 0 step 700.0 loss: 4.743049621582031
P2] Epoch 0 step 700.0 loss: 4.60902214050293
P3] Epoch 0 step 700.0 loss: 4.828198432922363
P0] Epoch 0 step 800.0 loss: 4.553040027618408
P1] Epoch 0 step 800.0 loss: 4.589711666107178
P3] Epoch 0 step 800.0 loss: 4.6373677253723145
P2] Epoch 0 step 800.0 loss: 4.353734493255615
P0] Epoch 0 step 900.0 loss: 4.054868698120117
P1] Epoch 0 step 900.0 loss: 4.2044997215271
P3] Epoch 0 step 900.0 loss: 4.340758323669434
P2] Epoch 0 step 900.0 loss: 4.305133819580078
P1] Epoch 0 step 1000.0 loss: 4.076944351196289
P3] Epoch 0 step 1000.0 loss: 4.375702857971191
P0] Epoch 0 step 1000.0 loss: 4.169274806976318
P2] Epoch 0 step 1000.0 loss: 4.172454833984375
P0] Epoch 0 step 1100.0 loss: 3.8013405799865723
P1] Epoch 0 step 1100.0 loss: 4.234490394592285
P3] Epoch 0 step 1100.0 loss: 4.091911792755127
P2] Epoch 0 step 1100.0 loss: 3.8987679481506348
P1] Epoch 0 step 1200.0 loss: 3.8100600242614746
P3] Epoch 0 step 1200.0 loss: 3.6621947288513184
P0] Epoch 0 step 1200.0 loss: 3.892784357070923
P2] Epoch 0 step 1200.0 loss: 3.9855384826660156
P0] Epoch 0 step 1300.0 loss: 3.7323787212371826
P3] Epoch 0 step 1300.0 loss: 3.8770270347595215
P1] Epoch 0 step 1300.0 loss: 3.706922769546509
P2] Epoch 0 step 1300.0 loss: 3.7966268062591553
P1] Epoch 0 step 1400.0 loss: 3.3876514434814453
P0] Epoch 0 step 1400.0 loss: 3.946122884750366
P3] Epoch 0 step 1400.0 loss: 3.747185707092285
P2] Epoch 0 step 1400.0 loss: 3.907095193862915
P2] Epoch 0 step 1500.0 loss: 3.5553112030029297
P1] Epoch 0 step 1500.0 loss: 3.715078830718994
P3] Epoch 0 step 1500.0 loss: 3.6275601387023926
P0] Epoch 0 step 1500.0 loss: 3.6622023582458496
P0] Epoch 0 step 1600.0 loss: 3.415205717086792
P1] Epoch 0 step 1600.0 loss: 3.476555109024048
P2] Epoch 0 step 1600.0 loss: 3.416653633117676
P3] Epoch 0 step 1600.0 loss: 3.6122629642486572
P0] Epoch 0 loss: 3.524609327316284  time: 1.51s
P2] Epoch 0 loss: 3.4264168739318848  time: 1.50s
P1] Epoch 0 loss: 3.156198263168335  time: 1.50s
P3] Epoch 0 loss: 3.507007122039795  time: 1.44s
P1] model saved
P0] model saved
P3] model saved
P2] model saved
I0124 04:11:14.507934 140626405369664 train.py:714] Evaluating on train set:
I0124 04:11:15.143818 140626405369664 train.py:493] Accuracy: 36.30%
I0124 04:11:15.147401 140626405369664 train.py:716] Evaluating on test set:
I0124 04:11:15.314341 140626405369664 train.py:493] Accuracy: 32.28%
I0124 04:11:15.315704 140626405369664 train.py:720] ----------- Tensor Parallel Row Linear ------------
Rank1 04:11:17.517413] Starting tp_main_fn rank 1 of world_size 4 ...
Rank2 04:11:17.518303] Starting tp_main_fn rank 2 of world_size 4 ...
Rank0 04:11:17.547637] Starting tp_main_fn rank 0 of world_size 4 ...
Rank3 04:11:17.575317] Starting tp_main_fn rank 3 of world_size 4 ...
Rank0 04:11:24.516916] Epoch 0 step 0.0 loss: 6.999758720397949
Rank1 04:11:24.516971] Epoch 0 step 0.0 loss: 6.999758720397949
Rank3 04:11:24.528961] Epoch 0 step 0.0 loss: 6.999758720397949
Rank2 04:11:24.530100] Epoch 0 step 0.0 loss: 6.999758720397949
Rank0 04:11:26.599261] Epoch 0 step 100.0 loss: 6.006659507751465
Rank3 04:11:26.600303] Epoch 0 step 100.0 loss: 6.006659507751465
Rank1 04:11:26.602268] Epoch 0 step 100.0 loss: 6.006659507751465
Rank2 04:11:26.607336] Epoch 0 step 100.0 loss: 6.006659507751465
Rank0 04:11:28.673087] Epoch 0 step 200.0 loss: 5.989320278167725
Rank3 04:11:28.673237] Epoch 0 step 200.0 loss: 5.989320278167725
Rank2 04:11:28.674817] Epoch 0 step 200.0 loss: 5.989320278167725
Rank1 04:11:28.678971] Epoch 0 step 200.0 loss: 5.989320278167725
Rank1 04:11:30.722528] Epoch 0 step 300.0 loss: 5.916282653808594
Rank0 04:11:30.724618] Epoch 0 step 300.0 loss: 5.916282653808594
Rank3 04:11:30.726455] Epoch 0 step 300.0 loss: 5.916282653808594
Rank2 04:11:30.731566] Epoch 0 step 300.0 loss: 5.916282653808594
Rank2 04:11:32.768758] Epoch 0 step 400.0 loss: 5.878521919250488
Rank3 04:11:32.775030] Epoch 0 step 400.0 loss: 5.878521919250488
Rank1 04:11:32.776638] Epoch 0 step 400.0 loss: 5.878521919250488
Rank0 04:11:32.776835] Epoch 0 step 400.0 loss: 5.878521919250488
Rank0 04:11:33.189370] Epoch 0 loss: 5.925197124481201  time: 13.41s
Rank2 04:11:33.189939] Epoch 0 loss: 5.925197124481201  time: 13.42s
Rank3 04:11:33.190029] Epoch 0 loss: 5.925197124481201  time: 15.00s
Rank1 04:11:33.191854] Epoch 0 loss: 5.925197124481201  time: 13.43s
Rank0 04:11:33.198314] model saved
Rank2 04:11:33.198810] model saved
Rank3 04:11:33.199218] model saved
Rank1 04:11:33.200150] model saved
I0124 04:11:34.134936 140626405369664 train.py:738] ----------- Tensor Parallel Customized Row Linear ------------
Rank0 04:11:36.235353] Starting tp_main_fn rank 0 of world_size 4 ...
Rank1 04:11:36.273173] Starting tp_main_fn rank 1 of world_size 4 ...
Rank3 04:11:36.290460] Starting tp_main_fn rank 3 of world_size 4 ...
Rank2 04:11:36.327823] Starting tp_main_fn rank 2 of world_size 4 ...
Rank1 04:11:42.485492] Epoch 0 step 0.0 loss: 6.859875202178955
Rank3 04:11:42.493250] Epoch 0 step 0.0 loss: 6.859875202178955
Rank0 04:11:42.494044] Epoch 0 step 0.0 loss: 6.859875202178955
Rank2 04:11:42.495030] Epoch 0 step 0.0 loss: 6.859875202178955
Rank2 04:11:45.493363] Epoch 0 step 100.0 loss: 6.033849239349365
Rank1 04:11:45.497170] Epoch 0 step 100.0 loss: 6.033849239349365
Rank0 04:11:45.501670] Epoch 0 step 100.0 loss: 6.033849239349365
Rank3 04:11:45.503115] Epoch 0 step 100.0 loss: 6.033849239349365
Rank0 04:11:48.489669] Epoch 0 step 200.0 loss: 6.041324615478516
Rank1 04:11:48.490334] Epoch 0 step 200.0 loss: 6.041324615478516
Rank3 04:11:48.496737] Epoch 0 step 200.0 loss: 6.041324615478516
Rank2 04:11:48.498082] Epoch 0 step 200.0 loss: 6.041324615478516
Rank2 04:11:51.478075] Epoch 0 step 300.0 loss: 6.069324970245361
Rank1 04:11:51.481121] Epoch 0 step 300.0 loss: 6.069324970245361
Rank3 04:11:51.482179] Epoch 0 step 300.0 loss: 6.069324970245361
Rank0 04:11:51.487781] Epoch 0 step 300.0 loss: 6.069324970245361
Rank3 04:11:54.488977] Epoch 0 step 400.0 loss: 5.934309005737305
Rank0 04:11:54.489249] Epoch 0 step 400.0 loss: 5.934309005737305
Rank1 04:11:54.493204] Epoch 0 step 400.0 loss: 5.934309005737305
Rank2 04:11:54.498563] Epoch 0 step 400.0 loss: 5.934309005737305
Rank1 04:11:55.084882] Epoch 0 loss: 5.974762439727783  time: 17.57s
Rank3 04:11:55.086922] Epoch 0 loss: 5.974762439727783  time: 17.57s
Rank0 04:11:55.091333] Epoch 0 loss: 5.974762439727783  time: 17.45s
Rank1 04:11:55.092538] model saved
Rank2 04:11:55.094144] Epoch 0 loss: 5.974762439727783  time: 17.50s
Rank3 04:11:55.095707] model saved
Rank0 04:11:55.099956] model saved
Rank2 04:11:55.102033] model saved
I0124 04:11:56.047648 140626405369664 train.py:756] ----------- Tensor Parallel Column Linear ------------
Rank2 04:11:58.325660] Starting tp_main_fn rank 2 of world_size 4 ...
Rank0 04:11:58.340025] Starting tp_main_fn rank 0 of world_size 4 ...
Rank1 04:11:58.348843] Starting tp_main_fn rank 1 of world_size 4 ...
Rank3 04:11:58.391133] Starting tp_main_fn rank 3 of world_size 4 ...
Rank3 04:12:05.156070] Epoch 0 step 0.0 loss: 6.625595569610596
Rank2 04:12:05.161372] Epoch 0 step 0.0 loss: 6.625595569610596
Rank1 04:12:05.163577] Epoch 0 step 0.0 loss: 6.625595569610596
Rank0 04:12:05.163790] Epoch 0 step 0.0 loss: 6.625595569610596
Rank3 04:12:10.802123] Epoch 0 step 100.0 loss: 6.354030609130859
Rank0 04:12:10.802241] Epoch 0 step 100.0 loss: 6.354030609130859
Rank2 04:12:10.808204] Epoch 0 step 100.0 loss: 6.354030609130859
Rank1 04:12:10.811267] Epoch 0 step 100.0 loss: 6.354030609130859
Rank3 04:12:16.467395] Epoch 0 step 200.0 loss: 6.6382646560668945
Rank2 04:12:16.468879] Epoch 0 step 200.0 loss: 6.6382646560668945
Rank1 04:12:16.476954] Epoch 0 step 200.0 loss: 6.6382646560668945
Rank0 04:12:16.477081] Epoch 0 step 200.0 loss: 6.6382646560668945
Rank2 04:12:22.106499] Epoch 0 step 300.0 loss: 6.710416793823242
Rank0 04:12:22.112824] Epoch 0 step 300.0 loss: 6.710416793823242
Rank3 04:12:22.113865] Epoch 0 step 300.0 loss: 6.710416793823242
Rank1 04:12:22.116379] Epoch 0 step 300.0 loss: 6.710416793823242
Rank2 04:12:27.763587] Epoch 0 step 400.0 loss: 7.011499881744385
Rank0 04:12:27.767269] Epoch 0 step 400.0 loss: 7.011499881744385
Rank1 04:12:27.767904] Epoch 0 step 400.0 loss: 7.011499881744385
Rank3 04:12:27.772405] Epoch 0 step 400.0 loss: 7.011499881744385
Rank1 04:12:28.888119] Epoch 0 loss: 7.011582851409912  time: 29.73s
Rank0 04:12:28.891164] Epoch 0 loss: 7.011582851409912  time: 28.42s
Rank3 04:12:28.892226] Epoch 0 loss: 7.011582851409912  time: 29.67s
Rank2 04:12:28.895237] Epoch 0 loss: 7.011582851409912  time: 28.44s
Rank1 04:12:28.896044] model saved
Rank0 04:12:28.900546] model saved
Rank3 04:12:28.901930] model saved
Rank2 04:12:28.904161] model saved
I0124 04:12:29.853090 140626405369664 train.py:773] ----------- Done -----------